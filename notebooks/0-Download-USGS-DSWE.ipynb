{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook downloads coincident ALOS SAR images USGS DSWE data for this project. The ALOS images are limited to high-resolution, RTC corrected, Fine Beam Dual pol (FBD) data that are acquired over North America. The USGS DSWE data is limited to the INterpreted layer With All Masking applied (INWAM) product, containing and obtained within +/- 1 days of a given SAR acquisition, containing less than 30% cloud cover, and at least 5% water surfaces.\n",
    "\n",
    "The code in this notebook performs the following search:\n",
    "\n",
    "> Query the ASF DAAC (via Vertex) for ALOS PALSAR data over the United States, filtering for RTC corrected FBD data <br>\n",
    ">>**Iterate over returned ALOS scenes**<br>\n",
    ">>>For a given ALOS PALSAR scene, find overlapping DSWE results within +/- 1 days of the SAR acquisition and having less than 30% cloud cover<br>\n",
    "\n",
    ">>>If no DSWE data meets this criteria, move to the next ALOS scene<br>\n",
    "\n",
    ">>>If overlapping DSWE data is available, verify that the overlap between the SAR scene and the DSWE data contains at least 50% non-cloud data, and at least 5% water<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASF and stac API libraries\n",
    "import asf_search as asf\n",
    "from pystac_client import Client\n",
    "import pystac\n",
    "\n",
    "# gis libraries\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, box\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import transform_bounds\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "# math imports\n",
    "import numpy as np\n",
    "\n",
    "# misc libraries\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from dateutil.tz import tzutc\n",
    "import netrc\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Union, Iterable\n",
    "import os\n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will look for ALOS-1 data over the United States\n",
    "aoi_file = Path('../data/world_country_shapefiles/ne_110m_admin_0_countries.shp')\n",
    "assert aoi_file.exists(), \"Missing AOI file\"\n",
    "world = gpd.read_file(aoi_file)\n",
    "usa_shape_wkt = world[world.SOVEREIGNT==\"United States of America\"].iloc[0].geometry.wkt\n",
    "usa_shape_bounding_wkt = box(*world[world.SOVEREIGNT==\"United States of America\"].iloc[0].geometry.bounds).wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up folder structure\n",
    "sar_output_path = Path('../data/scenes')\n",
    "sar_output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_coordinates(c_list):\n",
    "    '''\n",
    "    Given UMM extent results from an asf_search entry, we parse the list and return a shape\n",
    "    '''\n",
    "    coordinates = []\n",
    "    for c in c_list:\n",
    "        coordinates.append([c['Longitude'], c['Latitude']])\n",
    "\n",
    "    return shape({'type':'Polygon', 'coordinates':[coordinates]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_nearest_dswe_search(result):\n",
    "    '''\n",
    "    For an ALOS acquisition (returned from an ASF search), return a list of USGS DSWE (INWAM) of scene names that \n",
    "    overlap the acquisition and meet the search criteria (less than 30% cloud cover, within +/- 1 days of acquisition.\n",
    "    Collate results by acquisition date and return list sorted by increasing timedelta from SAR acquisition date. \n",
    "    Return an empty list if no DSWE results meet the search criteria\n",
    "    '''\n",
    "    # geometry, startTime = shape(result['geometry']), result['properties']['startTime']\n",
    "    geometry = return_coordinates(result['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons'][0]['Boundary']['Points'])\n",
    "    startTime = result['TemporalExtent']['RangeDateTime']['BeginningDateTime']\n",
    "\n",
    "    year, month, day = (int(x) for x in startTime.split('T')[0].split('-'))\n",
    "    ref_date = datetime.datetime(year=year, month=month, day=day, tzinfo=tzutc())\n",
    "\n",
    "    start_day = ref_date - datetime.timedelta(days=1)\n",
    "    end_day = ref_date + datetime.timedelta(days=1)\n",
    "\n",
    "    search_date_str = f\"{start_day.strftime('%Y-%m-%d')}/{end_day.strftime('%Y-%m-%d')}\"\n",
    "    print(f\"Acquisition date: {ref_date}, search range: {search_date_str}\")\n",
    "    usgs_stac_url = 'https://landsatlook.usgs.gov/stac-server'\n",
    "    catalog = Client.open(usgs_stac_url)\n",
    "\n",
    "    opts = {\n",
    "    'intersects' : geometry,\n",
    "    'collections':'landsat-c2l3-dswe',\n",
    "    'datetime' : search_date_str,\n",
    "    'max_items' : 100,\n",
    "    'query':{\n",
    "        'eo:cloud_cover':{'lt': 30},\n",
    "            }\n",
    "    }\n",
    "\n",
    "    search = catalog.search(**opts)\n",
    "    items = search.item_collection()\n",
    "    \n",
    "    # group the results together by acquisition date\n",
    "    # A single ALOS acquisition may correspond to multiple DSWE files\n",
    "    def collate_results(results):\n",
    "        collated_dict = defaultdict(list)\n",
    "        for r in results:\n",
    "            if r.assets['inwam'].href not in collated_dict[r.datetime]:\n",
    "                collated_dict[r.datetime].append(r.assets['inwam'].href)\n",
    "\n",
    "        return collated_dict\n",
    "    \n",
    "    # Sort by timedelta and return the nearest result (before or after reference date)\n",
    "    items = collate_results(items)\n",
    "\n",
    "    if len(items) > 0:\n",
    "        sorted_keys = sorted(items.keys(), key=lambda x:abs((x-ref_date).days))   \n",
    "        return items[sorted_keys[0]]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download a USGS asset\n",
    "def download_asset(item:Union[str, Iterable[str]], download_path:str='.'):\n",
    "\n",
    "    if type(item) is not list : item = [item]\n",
    "\n",
    "    download_path = Path(download_path)\n",
    "    download_path.mkdir(exist_ok = True)\n",
    "    \n",
    "    creds = netrc.netrc()\n",
    "    user,account,password = creds.authenticators('ers.cr.usgs.gov')\n",
    "\n",
    "    url = 'https://ers.cr.usgs.gov/login'\n",
    "    with requests.Session() as s:\n",
    "        \n",
    "        r = s.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser') \n",
    "        sval = soup.find('input', attrs={'name':'csrf'})['value']\n",
    "\n",
    "        data = {\"username\": user, \n",
    "            \"password\": password,\n",
    "            \"csrf\": sval}\n",
    "\n",
    "        bf = s.post(url, data = data)\n",
    "\n",
    "        downloaded_filepaths = []\n",
    "        for _item in item:\n",
    "            filename = _item.split('/')[-1]\n",
    "            \n",
    "            respb, count = None, 0\n",
    "            \n",
    "            while respb != 200 and count < 10:\n",
    "                respb = s.get(_item,\n",
    "                            allow_redirects=True,\n",
    "                            headers = {'content-type': 'image/tiff'})\n",
    "\n",
    "                with open(Path(download_path) / filename, 'wb') as src:\n",
    "                    src.write(respb.content)\n",
    "\n",
    "                count += 1\n",
    "\n",
    "            downloaded_filepaths.append(Path(download_path) / filename)\n",
    "    \n",
    "    return downloaded_filepaths\n",
    "\n",
    "# Function that returns % of valid pixels in a raster\n",
    "def return_pixel_stats(filepaths, bounds, cloud_val=9):\n",
    "    \n",
    "    crs = CRS.from_epsg(4326)\n",
    "    \n",
    "    try:\n",
    "        with rasterio.open(filepaths[0]) as ds:\n",
    "            nodata = ds.profile['nodata']\n",
    "            dst_crs = ds.crs\n",
    "    except:\n",
    "        return 0, 0\n",
    "\n",
    "\n",
    "    bounds = transform_bounds(crs, dst_crs, *bounds)\n",
    "    merged_raster, _ = merge(filepaths, bounds=bounds, nodata=nodata)\n",
    "\n",
    "    valid_fraction = 1 - (np.sum(merged_raster == nodata) + np.sum(merged_raster == cloud_val))/merged_raster.size\n",
    "    water_fraction = (np.sum(merged_raster == 1) + np.sum(merged_raster == 2))/merged_raster.size\n",
    "\n",
    "    return valid_fraction, water_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.datetime(year=2006, month=1, day=1, hour=0, minute=0, second=0)\n",
    "time_delta = datetime.timedelta(days=60) # we will make 2 month searches with ASF\n",
    "results = []\n",
    "\n",
    "for _ in range(38):\n",
    "    alos_opts = {\n",
    "                'maxResults':2000,\n",
    "                'platform':asf.PLATFORM.ALOS, \n",
    "                'instrument':asf.INSTRUMENT.PALSAR, \n",
    "                'processingLevel':asf.PRODUCT_TYPE.RTC_HIGH_RES,\n",
    "                'polarization':asf.POLARIZATION.HH_HV, \n",
    "                'beamMode':asf.BEAMMODE.FBD, \n",
    "                'intersectsWith':usa_shape_wkt,\n",
    "                'start':start_time.strftime(\"%Y-%m-%d\"),\n",
    "                'end':(start_time+time_delta).strftime(\"%Y-%m-%d\")\n",
    "                \n",
    "            }\n",
    "\n",
    "    start_time += time_delta\n",
    "    pages = asf.search_generator(**alos_opts)\n",
    "\n",
    "    for page in pages:\n",
    "        results.extend([product.umm for product in page])\n",
    "\n",
    "print(f\"Number of search results: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30743 results previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track ALOS scenes that have already been downloaded \n",
    "already_downloaded = [x.name for x in sar_output_path.iterdir() if x.is_dir()]\n",
    "downloaded_alos_paths = []\n",
    "\n",
    "for i, r in enumerate(results):\n",
    "    valid_data_percentage, water_data_percentage = 0, 0\n",
    "    filename = r['RelatedUrls'][0]['URL'].split('/')[-1]\n",
    "    if r['RelatedUrls'][0]['URL'].split('/')[-1][:-4] in already_downloaded:\n",
    "        continue\n",
    "\n",
    "    r_shape = return_coordinates(r['SpatialExtent']['HorizontalSpatialDomain']['Geometry']['GPolygons'][0]['Boundary']['Points'])\n",
    "    \n",
    "    dswe_results = return_nearest_dswe_search(r)\n",
    "    if len(dswe_results) == 0: # if there are no intersecting DSWE results\n",
    "        print(f\"no tiles for r[{i}]\")\n",
    "        continue\n",
    "    else:\n",
    "        # Download ASF file and unzip, download DSWE data\n",
    "        filepaths = download_asset(dswe_results)\n",
    "        valid_data_percentage, water_data_percentage = return_pixel_stats(filepaths, shape(r_shape).bounds)\n",
    "        \n",
    "        # Download ALOS acquisition zip file and extract\n",
    "        if (valid_data_percentage >= 0.5) and (water_data_percentage >= 0.05):\n",
    "            # r.download(sar_output_path)\n",
    "            to_download = asf.granule_search(r['GranuleUR'])[0]\n",
    "            to_download.download(sar_output_path)\n",
    "            _downloaded_file = sar_output_path/filename\n",
    "            assert _downloaded_file.exists(), 'Error, file does not exist'\n",
    "            with zipfile.ZipFile(_downloaded_file) as f:\n",
    "                f.extractall(sar_output_path)\n",
    "            \n",
    "            # delete zip file\n",
    "            _downloaded_file.unlink()\n",
    "            \n",
    "            # create subfolder for dswe data and move downloaded DSWE data to it\n",
    "            alos_path = (_downloaded_file).with_suffix('')\n",
    "            _usgs_folder_path = (alos_path/'usgs_dswe')\n",
    "            _usgs_folder_path.mkdir()\n",
    "            _ = [os.rename(str(x), _usgs_folder_path/x.name) for x in filepaths]\n",
    "\n",
    "            downloaded_alos_paths.append(alos_path.name)\n",
    "        \n",
    "        else: # if DSWE does not meet requirements, delete downloaded files\n",
    "            print(f\"Downloaded DSWE for r[{i}] did not have enough OW\")\n",
    "            _ = [x.unlink() for x in filepaths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alos_dswe_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now create a database of the downloaded ALOS and DSWE scenes so that users don't have to perform the very large search\n",
    "alos_scenes = [x.name for x in list(Path('../data/scenes').glob('*')) if (x.is_dir()) and ('AP_' in x.name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/scenes/')\n",
    "for alos_scene in alos_scenes:\n",
    "    usgs_path = data_path / alos_scene / 'usgs_dswe'\n",
    "    usgs_tiles = ' '.join([x.name for x in list(usgs_path.glob('*INWAM*.TIF'))])\n",
    "    alos_dswe_dict[alos_scene] = usgs_tiles\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'ALOS_scene':alos_dswe_dict.keys(), 'DSWE_tiles':alos_dswe_dict.values()}, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/alos_dswe_database.csv', index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('dl-with-alos-dswe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907c3b09d2d252a2f016f77c7afbbf74128c8fdc74ad06049d13802c1dfd5012"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
