{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook downloads coincident ALOS SAR images USGS DSWE data for this project. The ALOS images are limited to high-resolution, RTC corrected, Fine Beam Dual pol (FBD) data that overlap with provided AOIs. The USGS DSWE data is limited to the INterpreted layer With All Masking applied (INWAM) product, containing and obtained within +/- 7 days of a given SAR acquisition, containing at least 70% not-cloud data, and at least 5% water surfaces.\n",
    "\n",
    "Pseudocode for this notebook: \n",
    "\n",
    "**Iterate over AOIs**\n",
    "> Search ASF vertex for ALOS PALSAR data, filtering for RTC corrected FBD data <br>\n",
    ">>**Iterate over returned ASF results**<br>\n",
    ">>>For a given ALOS PALSAR scene, find overlapping DSWE results within +/- 7 days of the SAR acquisition and having less than 30% cloud cover<br>\n",
    "\n",
    ">>>If no DSWE data meets this criteria, move to the next ASF result for the same AOI<br>\n",
    "\n",
    ">>>If overlapping DSWE data is available, verify that the raster extent defined by the SAR scene contains at least 50% non-cloud data, and at least 5% water<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASF and stac API libraries\n",
    "import asf_search as asf\n",
    "from pystac_client import Client\n",
    "import pystac\n",
    "\n",
    "# gis libraries\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import shape, box\n",
    "import rasterio\n",
    "from rasterio.merge import merge\n",
    "from rasterio.warp import transform_bounds\n",
    "from rasterio.crs import CRS\n",
    "\n",
    "# math imports\n",
    "import numpy as np\n",
    "\n",
    "# misc libraries\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from dateutil.tz import tzutc\n",
    "import netrc\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Union, Iterable\n",
    "import os\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up folder structure\n",
    "sar_output_path = Path('../data/scenes')\n",
    "sar_output_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load file containing training AOIs\n",
    "aoi_file = Path('../data/training_scenes.geojson')\n",
    "assert aoi_file.exists(), \"Missing AOI file\"\n",
    "training_aois = gpd.read_file(aoi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLYGON ((-120.29750955041628 38.93464225417948, -119.76223334928117 40.14031434021133, -119.3465102351037 39.93081607794866, -119.87025589076036 38.7294744802862, -120.29750955041628 38.93464225417948))\n"
     ]
    }
   ],
   "source": [
    "for idx, aoi in training_aois.iterrows():\n",
    "    print(aoi.geometry.wkt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_nearest_dswe_search(result):\n",
    "    '''\n",
    "    For an ALOS acquisition (returned from an ASF search), return a list of USGS DSWE (INWAM) of scene names that \n",
    "    overlap the acquisition and meet the search criteria (less than 30% cloud cover, within +/- 7 days of acquisition.\n",
    "    Collate results by acquisition date and return list sorted by increasing timedelta from SAR acquisition date. \n",
    "    Return an empty list if no DSWE results meet the search criteria\n",
    "    '''\n",
    "    geometry, startTime = shape(result['geometry']), result['properties']['startTime']\n",
    "\n",
    "    year, month, day = (int(x) for x in startTime.split('T')[0].split('-'))\n",
    "    ref_date = datetime.datetime(year=year, month=month, day=day, tzinfo=tzutc())\n",
    "\n",
    "    start_day = ref_date - datetime.timedelta(days=7)\n",
    "    end_day = ref_date + datetime.timedelta(days=7)\n",
    "\n",
    "    search_date_str = f\"{start_day.strftime('%Y-%m-%d')}/{end_day.strftime('%Y-%m-%d')}\"\n",
    "    print(f\"Acquisition date: {ref_date}, search range: {search_date_str}\")\n",
    "    usgs_stac_url = 'https://landsatlook.usgs.gov/stac-server'\n",
    "    catalog = Client.open(usgs_stac_url)\n",
    "\n",
    "    opts = {\n",
    "    'intersects' : geometry,\n",
    "    'collections':'landsat-c2l3-dswe',\n",
    "    'datetime' : search_date_str,\n",
    "    'max_items' : 100,\n",
    "    'query':{\n",
    "        'eo:cloud_cover':{'lt': 30},\n",
    "            }\n",
    "    }\n",
    "\n",
    "    search = catalog.search(**opts)\n",
    "    items = search.get_all_items()\n",
    "    \n",
    "    # group the results together by acquisition date\n",
    "    # A single ALOS acquisition may correspond to multiple DSWE files\n",
    "    def collate_results(results):\n",
    "        collated_dict = defaultdict(list)\n",
    "        for r in results:\n",
    "            if r.assets['inwam'].href not in collated_dict[r.datetime]:\n",
    "                collated_dict[r.datetime].append(r.assets['inwam'].href)\n",
    "\n",
    "        return collated_dict\n",
    "    \n",
    "    # Sort by timedelta and return the nearest result (before or after reference date)\n",
    "    items = collate_results(items)\n",
    "\n",
    "    if len(items) > 0:\n",
    "        sorted_keys = sorted(items.keys(), key=lambda x:abs((x-ref_date).days))   \n",
    "        return items[sorted_keys[0]]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to download a USGS asset\n",
    "def download_asset(item:Union[str, Iterable[str]], download_path:str='.'):\n",
    "\n",
    "    if type(item) is not list : item = [item]\n",
    "\n",
    "    download_path = Path(download_path)\n",
    "    download_path.mkdir(exist_ok = True)\n",
    "    \n",
    "    creds = netrc.netrc()\n",
    "    user,account,password = creds.authenticators('ers.cr.usgs.gov')\n",
    "\n",
    "    url = 'https://ers.cr.usgs.gov/login'\n",
    "    with requests.Session() as s:\n",
    "        \n",
    "        r = s.get(url)\n",
    "        soup = BeautifulSoup(r.content, 'html.parser') \n",
    "        sval = soup.find('input', attrs={'name':'csrf'})['value']\n",
    "\n",
    "        data = {\"username\": user, \n",
    "            \"password\": password,\n",
    "            \"csrf\": sval}\n",
    "\n",
    "        bf = s.post(url, data = data)\n",
    "\n",
    "    downloaded_filepaths = []\n",
    "    for _item in item:\n",
    "        filename = _item.split('/')[-1]\n",
    "\n",
    "        respb = s.get(_item,\n",
    "                    allow_redirects=True,\n",
    "                    headers = {'content-type': 'image/tiff'})\n",
    "\n",
    "        with open(Path(download_path) / filename, 'wb') as src:\n",
    "            src.write(respb.content)\n",
    "\n",
    "        downloaded_filepaths.append(Path(download_path) / filename)\n",
    "    \n",
    "    return downloaded_filepaths\n",
    "\n",
    "# Function that returns % of valid pixels in a raster\n",
    "def return_pixel_stats(filepaths, bounds, cloud_val=9):\n",
    "    \n",
    "    crs = CRS.from_epsg(4326)\n",
    "    \n",
    "    with rasterio.open(filepaths[0]) as ds:\n",
    "        nodata = ds.profile['nodata']\n",
    "        dst_crs = ds.crs\n",
    "\n",
    "    bounds = transform_bounds(crs, dst_crs, *bounds)\n",
    "    merged_raster, _ = merge(filepaths, bounds=bounds, nodata=nodata)\n",
    "\n",
    "    valid_fraction = 1 - (np.sum(merged_raster == nodata) + np.sum(merged_raster == cloud_val))/merged_raster.size\n",
    "    water_fraction = (np.sum(merged_raster == 1) + np.sum(merged_raster == 2))/merged_raster.size\n",
    "\n",
    "    return valid_fraction, water_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track ALOS scenes that have already been downloaded \n",
    "already_downloaded = [x.name for x in sar_output_path.iterdir() if x.is_dir()]\n",
    "\n",
    "# search parameters for RTC corrected data\n",
    "alos_opts = {'platform':asf.PLATFORM.ALOS, 'processingLevel':asf.PRODUCT_TYPE.RTC_HIGH_RES,'polarization':asf.POLARIZATION.HH_HV}\n",
    "downloaded_alos_paths = []\n",
    "\n",
    "for _, aoi in training_aois.iterrows():\n",
    "    print(f\"Finding overlapping DSWE data for {aoi['name']}\")\n",
    "    results = asf.geo_search(intersectsWith=aoi.geometry.wkt, **alos_opts)\n",
    "\n",
    "    for r in results:\n",
    "        valid_data_percentage, water_data_percentage = 0, 0\n",
    "        if r.geojson()['properties']['fileName'][:-4] in already_downloaded:\n",
    "            continue\n",
    "        \n",
    "        dswe_results = return_nearest_dswe_search(r.geojson())\n",
    "        if len(dswe_results) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # Download ASF file and unzip, download DSWE data\n",
    "            filepaths = download_asset(dswe_results)\n",
    "            valid_data_percentage, water_data_percentage = return_pixel_stats(filepaths, shape(r.geojson()['geometry']).bounds) \n",
    "            if (valid_data_percentage >= 0.5) and (water_data_percentage >= 0.05):\n",
    "                break\n",
    "            else:\n",
    "                _ = [x.unlink() for x in filepaths]\n",
    "                continue\n",
    "    \n",
    "    # if there was a valid ALOS and DSWE combination, valid_data_percentage should be >= 0.5\n",
    "    if (valid_data_percentage >= 0.5) and (water_data_percentage >= 0.05):\n",
    "        # Download ALOS acquisition zip file and extract\n",
    "        r.download(sar_output_path)\n",
    "        _downloaded_file = sar_output_path/r.geojson()['properties']['fileName']\n",
    "        assert _downloaded_file.exists(), 'Error, file does not exist'\n",
    "        with zipfile.ZipFile(_downloaded_file) as f:\n",
    "            f.extractall(sar_output_path)\n",
    "        \n",
    "        # delete zip file\n",
    "        _downloaded_file.unlink()\n",
    "        \n",
    "        # create subfolder for dswe data and move downloaded DSWE data to it\n",
    "        alos_path = (_downloaded_file).with_suffix('')\n",
    "        _usgs_folder_path = (alos_path/'usgs_dswe')\n",
    "        _usgs_folder_path.mkdir()\n",
    "        _ = [os.rename(str(x), _usgs_folder_path/x.name) for x in filepaths]\n",
    "\n",
    "        downloaded_alos_paths.append(alos_path.name)\n",
    "    else:\n",
    "        print(f\"Valid ALOS/DSWE pair unavailable for {aoi.geometry.wkt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print names of created folders\n",
    "print(downloaded_alos_paths)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('playground')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37a3d299aef8aea26e51c3187081508fadf567cc05aaa145abe616e39d41c0d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
