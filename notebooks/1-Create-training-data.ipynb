{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ALOS PALSAR and co-incident USGS DSWE data downloaded in the previous notebook, we need to generate training data for our convolutional neural network.\n",
    "\n",
    "For this, we additionally require [Global Forest Change (GFC)](https://storage.googleapis.com/earthenginepartners-hansen/GFC-2020-v1.8/download.html) data and Copernicus GLO-30 DEM derived Height Above Nearest Drainage (HAND) data, which we download for each ALOS scene. \n",
    "\n",
    "We then split each scene into 512x512 non-overlapping chips that can be stacked and provided as inputs to our model. Our input stack contains the following data channels: [HH, HV, RED, NIR, SWIR1, SWIR2, DEM, HAND], and are mapped to the corresponding DSWE labels. In the DSWE data, we retain the following three classes: \"not water\", \"high confidence open surface water\" and \"moderate confidence open surface water\". The two water classes are combined into a single open surface water class.\n",
    "\n",
    "Any input stack which maps to a label array containing less than 10% valid data is discarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # GIS imports\n",
    "import rasterio\n",
    "from rasterio.warp import transform_bounds\n",
    "\n",
    "# scikit / numpy / pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Misc imports\n",
    "import random\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from collections import defaultdict\n",
    "\n",
    "# cutlery imports\n",
    "from tools import retrieve_hansen_mosaic, return_windowed_merge, denoise, return_slice_list, get_cropped_profile, return_nodata_mask, retrieve_hand_data\n",
    "\n",
    "# set for repeatable experiments\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('../data/scenes')\n",
    "alos_folders = [x.name for x in list(data_path.glob('AP*')) if x.is_dir()]\n",
    "print(f\"Number of ALOS scenes: {len(alos_folders)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path('../data/training_data/chips')\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# setup folders\n",
    "chip_types = ['hh', 'hv', 'red', 'nir', 'swir1', 'swir2', 'dem', 'hand', 'labels']\n",
    "chip_paths = []\n",
    "for c in chip_types:\n",
    "    (output_path/c).mkdir(exist_ok=True)\n",
    "    chip_paths.append(output_path/c)\n",
    "\n",
    "chip_path_dict = dict(zip(chip_types, chip_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chips_list = defaultdict(list)\n",
    "\n",
    "for scene in alos_folders:\n",
    "    print(scene)\n",
    "\n",
    "    hh_file = list((data_path/scene).glob('*HH*'))[0]\n",
    "    hv_file = list((data_path/scene).glob('*HV*'))[0]\n",
    "    dem_file = list((data_path/scene).glob('*dem*'))[0]\n",
    "\n",
    "    # we dealt with downloading the dswe data in a separate notebook\n",
    "    usgs_dswe_files = list((data_path/scene/'usgs_dswe').glob('*.TIF'))\n",
    "\n",
    "    with rasterio.open(hh_file) as ds:\n",
    "        hh_img = ds.read(1)\n",
    "        sar_bounds = ds.bounds \n",
    "        sar_profile = ds.profile \n",
    "        sar_crs = ds.crs\n",
    "    \n",
    "    with rasterio.open(hv_file) as ds:\n",
    "        hv_img = ds.read(1)\n",
    "    \n",
    "    with rasterio.open(dem_file) as ds:\n",
    "        dem_img = ds.read(1)\n",
    "        dem_profile = ds.profile\n",
    "\n",
    "    with rasterio.open(usgs_dswe_files[0]) as ds:\n",
    "        dswe_crs = ds.crs\n",
    "        dswe_profile = ds.profile\n",
    "\n",
    "    # Let's retrieve the Hansen tiles overlapping the SAR scene\n",
    "    sar_bounds_4326 = transform_bounds(sar_crs.to_epsg(), 4326, *sar_bounds)\n",
    "    hansen_files = retrieve_hansen_mosaic(sar_bounds_4326, data_product = 'first', download_path=Path('../data/hand_data/'))\n",
    "\n",
    "    hansen_img, hansen_profile = return_windowed_merge(hansen_files, sar_bounds_4326, sar_profile)\n",
    "\n",
    "    mask = return_nodata_mask([hh_img, hv_img], nodata=0)\n",
    "    mask += return_nodata_mask([hansen_img[0]], hansen_profile['nodata'])\n",
    "\n",
    "    # Obtain HAND data\n",
    "    hand_files = retrieve_hand_data(sar_bounds_4326, download_path=Path('../data/hansen_mosaics/'))\n",
    "    hand_img, hand_profile = return_windowed_merge(hand_files, sar_bounds_4326, sar_profile)\n",
    "    hand_img = np.squeeze(hand_img)\n",
    "\n",
    "    # treat the dswe img for use as labels\n",
    "    # Refer to the document 'Landsat Collection 2 Level 3 Dynamic Surface Water Extent Data Format Control Book' for more information\n",
    "    sar_bounds_dswe_crs = transform_bounds(sar_crs, dswe_crs, *sar_bounds)\n",
    "    labels, labels_profile = return_windowed_merge(usgs_dswe_files, sar_bounds_dswe_crs, sar_profile)\n",
    "    labels = np.squeeze(labels)\n",
    "\n",
    "    # mask += return_nodata_mask([labels], nodata=255)\n",
    "\n",
    "    # Mask out no data regions\n",
    "    mask = np.where(mask>0, 0, 1).astype('uint8')\n",
    "    hh_img *= mask\n",
    "    hv_img *= mask\n",
    "    hansen_img *= mask\n",
    "    dem_img *= mask\n",
    "    hand_img *= mask\n",
    "    \n",
    "    idx1 = np.where(labels == 0)  # not water\n",
    "    idx2 = np.where((labels == 1) | (labels == 2)) # High and moderate confidence open water classes\n",
    "    idx3 = np.where(mask == 0)\n",
    "    idx4 = np.where((labels == 3) | (labels == 4)) # Conservative and aggressive partial surface water classes\n",
    "    \n",
    "    # The labeling used here is simply because it aligns well with backscatter signatures\n",
    "    # i.e., low back scatter = water surfaces (0), high back scatter = not water (1)\n",
    "    # All other pixels are labeled 255, which will correspond to a 'don't care' value during training\n",
    "    labels = 255 + np.zeros(np.squeeze(hh_img).shape, dtype=dswe_profile['dtype'])\n",
    "    labels[idx2] = 0\n",
    "    labels[idx1] = 1\n",
    "    labels[idx3] = 255\n",
    "    labels[idx4] = 255 # we do not want to score the model over classifications of PSW pixels\n",
    "\n",
    "    chip_prefix = f\"AP_{scene[3:8]}{scene[14:18]}\"\n",
    "\n",
    "    image_dict = {\n",
    "        'hh': (hh_img, sar_profile['nodata'], sar_profile['dtype']),\n",
    "        'hv': (hv_img, sar_profile['nodata'], sar_profile['dtype']),\n",
    "        'red': (hansen_img[0, ...], 0, 'int16'),\n",
    "        'nir': (hansen_img[1, ...], 0, 'int16'),\n",
    "        'swir1': (hansen_img[2, ...], 0, 'int16'),\n",
    "        'swir2': (hansen_img[3, ...], 0, 'int16'),\n",
    "        'dem': (dem_img, dem_profile['nodata'], dem_img.dtype),\n",
    "        'hand': (hand_img, hand_profile['nodata'], hand_img.dtype),\n",
    "        'labels': (labels, 255, 'uint8')\n",
    "    }\n",
    "    \n",
    "    # Write out labels for future reference\n",
    "    with rasterio.open(hh_file.parent/'labels.tif', 'w', **labels_profile) as ds:\n",
    "        ds.write(labels.reshape(1, *labels.shape).astype('uint8'))\n",
    "\n",
    "    # slice up the image into chips and iterate over slices\n",
    "    slice_list = return_slice_list(hh_img.shape, (512, 512))\n",
    "    \n",
    "    count = 0\n",
    "\n",
    "    for (y_slice, x_slice) in slice_list:\n",
    "        \n",
    "        label_chip = labels[y_slice, x_slice]\n",
    "\n",
    "        # Any time a SAR image has less than 10% data, we'll skip that chip\n",
    "        if np.sum(label_chip != 255)/(label_chip.size) < 0.1:\n",
    "            continue\n",
    "\n",
    "        current_filename = f\"{chip_prefix}_{str(count).zfill(5)}.tif\"\n",
    "        chip_profile = get_cropped_profile(labels_profile, x_slice, y_slice)\n",
    "\n",
    "        for _chip_type, _chip_output_path in chip_path_dict.items():\n",
    "            chip_profile['nodata'] = image_dict[_chip_type][1]\n",
    "            chip_profile['dtype'] = image_dict[_chip_type][2]\n",
    "            temp_chip = image_dict[_chip_type][0][y_slice, x_slice]\n",
    "            with rasterio.open(_chip_output_path / current_filename, 'w', **chip_profile) as ds:\n",
    "                ds.write(temp_chip.reshape(1, *temp_chip.shape))\n",
    "            \n",
    "            chips_list[_chip_type].append(_chip_output_path / current_filename)        \n",
    "        \n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(chips_list)\n",
    "df.to_csv(output_path/'training_data.csv')\n",
    "print(f\"Number of training samples: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('dl-with-alos-dswe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "907c3b09d2d252a2f016f77c7afbbf74128c8fdc74ad06049d13802c1dfd5012"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
