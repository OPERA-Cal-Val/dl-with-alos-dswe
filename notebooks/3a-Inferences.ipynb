{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have trained models capable of using SAR, and SAR+Optical data to make segmentation maps of open surface water. Let's see how the models perform on a previously unknown scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rasterio.warp import transform_bounds\n",
    "\n",
    "from rasterio.merge import merge\n",
    "\n",
    "from tools import retrieve_hansen_mosaic, return_windowed_merge, denoise, return_slice_list, get_cropped_profile, return_nodata_mask, retrieve_hand_data\n",
    "\n",
    "import torch\n",
    "from model import sarDataLoader, sarInferenceModel\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\" # Force kernel to use this GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_path = Path('../data/inference_scenes/AP_13874_FBD_F0580_RT1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a newly obtained scene, we need to still download corresponding Global Forest Watch and HAND data. We will then need to split these data into 512x512 pixel chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = scene_path / 'chips'\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# setup folders\n",
    "chip_types = ['hh', 'hv', 'red', 'nir', 'swir1', 'swir2', 'dem', 'hand']\n",
    "chip_paths = []\n",
    "for c in chip_types:\n",
    "    (output_path/c).mkdir(exist_ok=True)\n",
    "    chip_paths.append(output_path/c)\n",
    "\n",
    "chip_path_dict = dict(zip(chip_types, chip_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remake_chips = True\n",
    "\n",
    "chips_list = defaultdict(list)\n",
    "\n",
    "scene_name = scene_path.name\n",
    "\n",
    "hh_file = list(scene_path.glob('*HH*'))[0]\n",
    "hv_file = list(scene_path.glob('*HV*'))[0]\n",
    "dem_file = list(scene_path.glob('*dem*'))[0]\n",
    "\n",
    "with rasterio.open(hh_file) as ds:\n",
    "    hh_img = ds.read(1)\n",
    "    sar_bounds = ds.bounds \n",
    "    sar_profile = ds.profile \n",
    "    sar_crs = ds.crs\n",
    "\n",
    "with rasterio.open(hv_file) as ds:\n",
    "    hv_img = ds.read(1)\n",
    "\n",
    "with rasterio.open(dem_file) as ds:\n",
    "    dem_img = ds.read(1)\n",
    "    dem_profile = ds.profile\n",
    "\n",
    "# Let's retrieve the Hansen tiles overlapping the SAR scene\n",
    "sar_bounds_4326 = transform_bounds(sar_crs.to_epsg(), 4326, *sar_bounds)\n",
    "hansen_files = retrieve_hansen_mosaic(sar_bounds_4326, data_product = 'first', download_path=Path('../data/hansen_mosaics/'))\n",
    "\n",
    "hansen_img, hansen_profile = return_windowed_merge(hansen_files, sar_bounds_4326, sar_profile)\n",
    "\n",
    "mask = return_nodata_mask([hh_img, hv_img], nodata=0)\n",
    "mask += return_nodata_mask([hansen_img[0]], hansen_profile['nodata'])\n",
    "\n",
    "# Obtain HAND data\n",
    "hand_files = retrieve_hand_data(sar_bounds_4326, download_path=Path('../data/hand_data/'))\n",
    "hand_img, hand_profile = return_windowed_merge(hand_files, sar_bounds_4326, sar_profile)\n",
    "hand_img = np.squeeze(hand_img)\n",
    "\n",
    "# Mask out no data regions\n",
    "mask = np.where(mask>0, 0, 1).astype('uint8')\n",
    "hh_img *= mask\n",
    "hv_img *= mask\n",
    "hansen_img *= mask\n",
    "dem_img *= mask\n",
    "hand_img *= mask\n",
    "\n",
    "chip_prefix = f\"AP_{scene_name[3:8]}{scene_name[14:18]}\"\n",
    "\n",
    "image_dict = {\n",
    "    'hh': (hh_img, sar_profile['nodata'], sar_profile['dtype']),\n",
    "    'hv': (hv_img, sar_profile['nodata'], sar_profile['dtype']),\n",
    "    'red': (hansen_img[0, ...], 0, 'int16'),\n",
    "    'nir': (hansen_img[1, ...], 0, 'int16'),\n",
    "    'swir1': (hansen_img[2, ...], 0, 'int16'),\n",
    "    'swir2': (hansen_img[3, ...], 0, 'int16'),\n",
    "    'dem': (dem_img, dem_profile['nodata'], dem_img.dtype),\n",
    "    'hand': (hand_img, hand_profile['nodata'], hand_img.dtype),\n",
    "}\n",
    "\n",
    "# We specify a stride of 128 so that bad inferences near chip edges can be minimized\n",
    "slice_list = return_slice_list(hh_img.shape, (512, 512), x_stride=128, y_stride=128)\n",
    "\n",
    "count = 0\n",
    "\n",
    "if remake_chips:\n",
    "    for (y_slice, x_slice) in slice_list:\n",
    "        \n",
    "        current_filename = f\"{chip_prefix}_{str(count).zfill(5)}.tif\"\n",
    "        chip_profile = get_cropped_profile(sar_profile, x_slice, y_slice)\n",
    "\n",
    "        for _chip_type, _chip_output_path in chip_path_dict.items():\n",
    "            chip_profile['nodata'] = image_dict[_chip_type][1]\n",
    "            chip_profile['dtype'] = image_dict[_chip_type][2]\n",
    "            temp_chip = image_dict[_chip_type][0][y_slice, x_slice]\n",
    "            with rasterio.open(_chip_output_path / current_filename, 'w', **chip_profile) as ds:\n",
    "                ds.write(temp_chip.reshape(1, *temp_chip.shape))\n",
    "            \n",
    "            chips_list[_chip_type].append(_chip_output_path / current_filename)        \n",
    "        \n",
    "        count += 1\n",
    "    df = pd.DataFrame(chips_list)\n",
    "    df.to_csv(output_path/'chips.csv')\n",
    "    print(f\"Number of chips: {len(df)}\")\n",
    "\n",
    "else:\n",
    "    df = pd.read_csv(output_path/'chips.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate inferences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAR only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"sarData\":True,\n",
    "    \"denoisingWeight\":0.35,\n",
    "    \"opticalData\":False,    \n",
    "    \"output_classes\":2,\n",
    "    \"backbone\" : \"resnet50\",\n",
    "    \"gpu\": True,\n",
    "    \"ngpus\":1,\n",
    "    \"experiment_name\" : \"sar_only_model\"\n",
    "}\n",
    "\n",
    "dataloader_params = {\n",
    "    \"return_sar\":True,\n",
    "    \"return_optical\":False,\n",
    "    \"denoising_weight\":.35,\n",
    "    \"return_optical\":False,\n",
    "    \"return_dem\":True,\n",
    "    \"return_hand\":True\n",
    "}\n",
    "\n",
    "model = sarInferenceModel(model_params)\n",
    "model.load_state_dict(torch.load(f\"model-weights/sar_only_model.pt\"), strict=False)\n",
    "model.model.cuda() # Load model into the GPU so that it can do batch processing of chips efficiently\n",
    "model.model.eval() # Put model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sarDataLoader(x_paths=df, y_paths=None, **dataloader_params)\n",
    "\n",
    "inference_path = scene_path / 'inferences'\n",
    "inference_path.mkdir(exist_ok=True)\n",
    "\n",
    "exp_name = 'sar_only_inferences'\n",
    "\n",
    "output_path = inference_path / exp_name\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "current_inferences = []\n",
    "count = 0\n",
    "\n",
    "def return_batch_indices(dataset_len, batch_size):\n",
    "    batch_indices = []\n",
    "    for i in range(0, dataset_len, batch_size):\n",
    "        indices = np.array(list(range(i, i+batch_size)))\n",
    "        indices = indices[indices<dataset_len]\n",
    "        batch_indices.append(indices)\n",
    "    \n",
    "    return batch_indices\n",
    "\n",
    "batch_idxs = return_batch_indices(len(df), batch_size=12)\n",
    "\n",
    "for batch_idx in batch_idxs:\n",
    "    batch = [dataset.__getitem__(i, inference=True) for i in batch_idx]\n",
    "    \n",
    "    img_batch = np.stack([_b[0] for _b in batch], axis = 0)\n",
    "    profile_batch = [_b[2] for _b in batch]\n",
    "    \n",
    "    img_batch = torch.Tensor(img_batch).cuda(non_blocking=True)\n",
    "    inferences = model.forward(img_batch).detach().cpu().numpy()\n",
    "\n",
    "    for n in np.arange(inferences.shape[0]):\n",
    "        inference = np.argmax(np.squeeze(inferences[n, ...]), axis=0)\n",
    "        inference_filename = output_path/f'inference_{str(count).zfill(5)}.tiff'\n",
    "\n",
    "        chip_profile = profile_batch[n]\n",
    "        chip_profile['nodata'] = -1\n",
    "        chip_profile['nodata'] = 'int8'\n",
    "\n",
    "        with rasterio.open(inference_filename, 'w', **chip_profile) as ds:\n",
    "            ds.write(inference.reshape(1, *inference.shape).astype('int8'))                                                             \n",
    "        count += 1\n",
    "        current_inferences.append(inference_filename)\n",
    "\n",
    "merged_inference, out_trans = merge(current_inferences, method='last')\n",
    "\n",
    "with rasterio.open(list(scene_path.glob(\"*HH*\"))[0]) as ds:\n",
    "    sar_profile = ds.profile\n",
    "    nodata_mask = ds.read() == ds.profile['nodata']\n",
    "\n",
    "merged_inference[nodata_mask] = -1\n",
    "inference_profile = sar_profile\n",
    "inference_profile['nodata'] = -1\n",
    "inference_profile['dtype'] = 'int8'\n",
    "with rasterio.open(output_path/f'{exp_name}_merged_inferences.tif', 'w', **inference_profile) as ds:\n",
    "    ds.write(merged_inference.astype('int8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SAR and optical inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"sarData\":True,\n",
    "    \"denoisingWeight\":0.35,\n",
    "    \"opticalData\":True,    \n",
    "    \"output_classes\":2,\n",
    "    \"backbone\" : \"resnet50\",\n",
    "    \"gpu\": True,\n",
    "    \"ngpus\":1,\n",
    "    \"experiment_name\" : \"sar_and_optical_model\"\n",
    "}\n",
    "\n",
    "dataloader_params = {\n",
    "    \"return_sar\":True,\n",
    "    \"return_optical\":True,\n",
    "    \"denoising_weight\":.35,\n",
    "    \"return_dem\":True,\n",
    "    \"return_hand\":True\n",
    "}\n",
    "\n",
    "model = sarInferenceModel(model_params)\n",
    "model.load_state_dict(torch.load(f\"model-weights/sar_and_optical_model.pt\"), strict=False)\n",
    "model.model.cuda() # Load model into the GPU so that it can do batch processing of chips efficiently\n",
    "model.model.eval() # Put model in evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sarDataLoader(x_paths=df, y_paths=None, **dataloader_params)\n",
    "\n",
    "inference_path = scene_path / 'inferences'\n",
    "inference_path.mkdir(exist_ok=True)\n",
    "\n",
    "exp_name = 'sar_and_optical_inferences'\n",
    "\n",
    "output_path = inference_path / exp_name\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "current_inferences = []\n",
    "count = 0\n",
    "\n",
    "def return_batch_indices(dataset_len, batch_size):\n",
    "    batch_indices = []\n",
    "    for i in range(0, dataset_len, batch_size):\n",
    "        indices = np.array(list(range(i, i+batch_size)))\n",
    "        indices = indices[indices<dataset_len]\n",
    "        batch_indices.append(indices)\n",
    "    \n",
    "    return batch_indices\n",
    "\n",
    "batch_idxs = return_batch_indices(len(df), batch_size=12)\n",
    "\n",
    "for batch_idx in batch_idxs:\n",
    "    batch = [dataset.__getitem__(i, inference=True) for i in batch_idx]\n",
    "    \n",
    "    img_batch = np.stack([_b[0] for _b in batch], axis = 0)\n",
    "    profile_batch = [_b[2] for _b in batch]\n",
    "    \n",
    "    img_batch = torch.Tensor(img_batch).cuda(non_blocking=True)\n",
    "    inferences = model.forward(img_batch).detach().cpu().numpy()\n",
    "\n",
    "    for n in np.arange(inferences.shape[0]):\n",
    "        inference = np.argmax(np.squeeze(inferences[n, ...]), axis=0)\n",
    "        inference_filename = output_path/f'inference_{str(count).zfill(5)}.tiff'\n",
    "\n",
    "        chip_profile = profile_batch[n]\n",
    "        chip_profile['nodata'] = -1\n",
    "        chip_profile['nodata'] = 'int8'\n",
    "\n",
    "        with rasterio.open(inference_filename, 'w', **chip_profile) as ds:\n",
    "            ds.write(inference.reshape(1, *inference.shape).astype('int8'))\n",
    "        count += 1\n",
    "        current_inferences.append(inference_filename)\n",
    "\n",
    "merged_inference, out_trans = merge(current_inferences, method='last')\n",
    "\n",
    "with rasterio.open(list(scene_path.glob(\"*HH*\"))[0]) as ds:\n",
    "    sar_profile = ds.profile\n",
    "    nodata_mask = ds.read() == ds.profile['nodata']\n",
    "\n",
    "merged_inference[nodata_mask] = -1\n",
    "inference_profile = sar_profile\n",
    "inference_profile['nodata'] = -1\n",
    "inference_profile['dtype'] = 'int8'\n",
    "with rasterio.open(output_path/f'{exp_name}_merged_inferences.tif', 'w', **inference_profile) as ds:\n",
    "    ds.write(merged_inference.astype('int8'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('dl-with-alos-dswe')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "687fac7ebddc94b5987a54dfd21d35444b99b04225b52bf978adb248e82bdec2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
